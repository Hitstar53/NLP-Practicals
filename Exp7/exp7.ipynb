{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tf  {border-collapse:collapse;border-spacing:0;width:100%}\n",
    ".tf td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tf th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tf .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    ".col1 { width: 20%;}\n",
    ".col2 { width: 80%;}\n",
    "</style>\n",
    "<table class=\"tf\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Name</th>\n",
    "    <th class=\"tg-0pky col2\">Hatim Sawai</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky col1\">UID No.</td>\n",
    "    <td class=\"tg-0pky col2\">2021300108</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky col1\">Experiment No.</td>\n",
    "    <td class=\"tg-0pky col2\">7</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align:center;font-weight:500;\">Experiment 7</p>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;width:100%}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 10px;word-break:normal;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top;}\n",
    ".col1 { width: 20%;}\n",
    ".col2 { width: 80%;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Aim</th>\n",
    "    <th class=\"tg-0pky col2\">Perform chunking by analyzing the importance of selecting proper features for training a model and size of training.</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installation of NLTK and downloading the required corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chunk import RegexpParser\n",
    "from prettytable import PrettyTable\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hatim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hatim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\hatim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hatim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the necessary nltk data including ne_chunk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the corpus and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The new study reveals shocking statistics abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The latest iPhone model exceeds expectations w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Customers are raving about the delicious flavo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Political tensions rise as negotiations betwee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Researchers announce a breakthrough in cancer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The new study reveals shocking statistics abou...\n",
       "1  The latest iPhone model exceeds expectations w...\n",
       "2  Customers are raving about the delicious flavo...\n",
       "3  Political tensions rise as negotiations betwee...\n",
       "4  Researchers announce a breakthrough in cancer ..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv\n",
    "df = pd.read_csv('../dataset/exp7.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    text = re.sub(r'\\d', '', text)  # Remove digits\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_mapping = {\n",
    "    'NP: {<DT>?<JJ>*<NN>}': 'noun phrase',\n",
    "    'PP: {<IN><NP>}': 'prepositional phrase',\n",
    "    'VP: {<VB.*><NP|PP|CLAUSE>+$}': 'verb phrase'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "df['text'] = df['text'].apply(preprocess)\n",
    "sentences = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform chunking using regular expressions\n",
    "def chunk_with_regex(sentence):\n",
    "    grammar = r\"\"\"\n",
    "        NP: {<DT>?<JJ>*<NN>}   # Chunk sequences of DT, JJ, NN\n",
    "        PP: {<IN><NP>}         # Chunk prepositions followed by NP\n",
    "        VP: {<VB.*><NP|PP>*}   # Chunk verbs followed by NP or PP\n",
    "        ADJP: {<JJ>+}          # Chunk sequences of JJ\n",
    "        ADVP: {<RB.*>}         # Chunk adverbs\n",
    "    \"\"\"\n",
    "    parser = RegexpParser(grammar)\n",
    "    parsed_sentence = parser.parse(sentence)\n",
    "    return parsed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform chunking using NLTK library\n",
    "def chunk_with_nltk(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "    grammar = r\"\"\"\n",
    "        NP: {<DT>?<JJ>*<NN>}    # Chunk sequences of DT, JJ, NN\n",
    "        PP: {<IN><NP>}          # Chunk prepositions followed by NP\n",
    "        VP: {<VB.*><NP|PP>*}    # Chunk verbs followed by NP or PP\n",
    "        ADJP: {<JJ>+}           # Chunk sequences of JJ\n",
    "        ADVP: {<RB.*>}          # Chunk adverbs\n",
    "    \"\"\"\n",
    "    parser = nltk.RegexpParser(grammar)\n",
    "    parsed_sentence = parser.parse(tagged_words)\n",
    "    return parsed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking using regular expressions\n",
      "+---------------------------------------------+------+\n",
      "|                    Chunk                    | Tag  |\n",
      "+---------------------------------------------+------+\n",
      "|                the new study                |  NP  |\n",
      "|                   shocking                  |  VP  |\n",
      "|                about climate                |  PP  |\n",
      "|                   climate                   |  NP  |\n",
      "|                    change                   |  NP  |\n",
      "|                    iphone                   |  NP  |\n",
      "|                    model                    |  NP  |\n",
      "|                   exceeds                   |  VP  |\n",
      "|                  innovative                 | ADJP |\n",
      "|                     are                     |  VP  |\n",
      "|                    raving                   |  VP  |\n",
      "|                  delicious                  | ADJP |\n",
      "|                   new ice                   |  NP  |\n",
      "|                    cream                    |  NP  |\n",
      "|                  political                  | ADJP |\n",
      "|                     rise                    |  VP  |\n",
      "|                    stall                    |  VP  |\n",
      "| announce a breakthrough in cancer treatment |  VP  |\n",
      "|                a breakthrough               |  NP  |\n",
      "|                  in cancer                  |  PP  |\n",
      "+---------------------------------------------+------+\n"
     ]
    }
   ],
   "source": [
    "# Perform chunking using regular expressions\n",
    "# Initialize PrettyTable\n",
    "chunk_table = PrettyTable([\"Chunk\", \"Tag\"])\n",
    "print(\"Chunking using regular expressions\")\n",
    "for sentence in sentences:\n",
    "    parsed_sentence = chunk_with_regex(pos_tag(word_tokenize(sentence)))\n",
    "    for subtree in parsed_sentence.subtrees():\n",
    "        if subtree.label() in ['NP', 'PP', 'VP', 'ADJP', 'ADVP']:\n",
    "            chunk_table.add_row([\" \".join(word for word, tag in subtree.leaves()), subtree.label()])\n",
    "\n",
    "# Print the table\n",
    "print(chunk_table[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunking using NLTK library:\n",
      "+---------------------------------------------+------+\n",
      "|                    Chunk                    | Tag  |\n",
      "+---------------------------------------------+------+\n",
      "|                the new study                |  NP  |\n",
      "|                   shocking                  |  VP  |\n",
      "|                about climate                |  PP  |\n",
      "|                   climate                   |  NP  |\n",
      "|                    change                   |  NP  |\n",
      "|                    iphone                   |  NP  |\n",
      "|                    model                    |  NP  |\n",
      "|                   exceeds                   |  VP  |\n",
      "|                  innovative                 | ADJP |\n",
      "|                     are                     |  VP  |\n",
      "|                    raving                   |  VP  |\n",
      "|                  delicious                  | ADJP |\n",
      "|                   new ice                   |  NP  |\n",
      "|                    cream                    |  NP  |\n",
      "|                  political                  | ADJP |\n",
      "|                     rise                    |  VP  |\n",
      "|                    stall                    |  VP  |\n",
      "| announce a breakthrough in cancer treatment |  VP  |\n",
      "|                a breakthrough               |  NP  |\n",
      "|                  in cancer                  |  PP  |\n",
      "+---------------------------------------------+------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform chunking using NLTK library\n",
    "chunk_table = PrettyTable([\"Chunk\", \"Tag\"])\n",
    "print(\"\\nChunking using NLTK library:\")\n",
    "for sentence in sentences:\n",
    "    parsed_sentence = chunk_with_nltk(sentence)\n",
    "    for subtree in parsed_sentence.subtrees():\n",
    "        if subtree.label() in ['NP', 'PP', 'VP', 'ADJP', 'ADVP']:\n",
    "            chunk_table.add_row([\" \".join(word for word, tag in subtree.leaves()), subtree.label()])\n",
    "            \n",
    "print(chunk_table[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barack Obama was the 44th President of the Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple Inc. is headquartered in Cupertino, Cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Eiffel Tower is located in Paris, France.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albert Einstein was a renowned physicist born ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Beatles were an influential band formed in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Barack Obama was the 44th President of the Uni...\n",
       "1  Apple Inc. is headquartered in Cupertino, Cali...\n",
       "2      The Eiffel Tower is located in Paris, France.\n",
       "3  Albert Einstein was a renowned physicist born ...\n",
       "4  The Beatles were an influential band formed in..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('../dataset/exp7a.csv')\n",
    "sentences = df1[\"text\"].tolist()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for Named Entity Recognition\n",
    "def ner_with_nltk(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    named_entities = ne_chunk(tagged_tokens)\n",
    "    \n",
    "    # Extract named entity labels\n",
    "    named_entities_labels = []\n",
    "    for subtree in named_entities:\n",
    "        if isinstance(subtree, nltk.Tree):\n",
    "            entity_label = subtree.label()\n",
    "            named_entities_labels.append(\n",
    "                (entity_label, \" \".join(word for word, tag in subtree.leaves()))\n",
    "            )\n",
    "    return named_entities_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entities:\n",
      "+--------------+----------------+\n",
      "|    Entity    |  Phrase Type   |\n",
      "+--------------+----------------+\n",
      "|    PERSON    |     Barack     |\n",
      "|    PERSON    |     Obama      |\n",
      "|     GPE      | United States  |\n",
      "|    PERSON    |     Apple      |\n",
      "| ORGANIZATION |      Inc.      |\n",
      "|     GPE      |   Cupertino    |\n",
      "|     GPE      |   California   |\n",
      "| ORGANIZATION |  Eiffel Tower  |\n",
      "|     GPE      |     Paris      |\n",
      "|     GPE      |     France     |\n",
      "|    PERSON    |     Albert     |\n",
      "|    PERSON    |    Einstein    |\n",
      "| ORGANIZATION |    Beatles     |\n",
      "|     GPE      |   Liverpool    |\n",
      "|     GPE      |    England     |\n",
      "|    PERSON    |     Google     |\n",
      "|    PERSON    |   Larry Page   |\n",
      "|    PERSON    |  Sergey Brin   |\n",
      "| ORGANIZATION |   Great Wall   |\n",
      "|     GPE      |     China      |\n",
      "|     GPE      |     China      |\n",
      "|    PERSON    |     Nelson     |\n",
      "|    PERSON    |    Mandela     |\n",
      "| ORGANIZATION |      NASA      |\n",
      "|     GPE      | New York City  |\n",
      "|    PERSON    |     Mount      |\n",
      "| ORGANIZATION |    Everest     |\n",
      "|    PERSON    |      Elon      |\n",
      "| ORGANIZATION |      Musk      |\n",
      "| ORGANIZATION | CEO of SpaceX  |\n",
      "|    PERSON    |     Tesla      |\n",
      "| ORGANIZATION |   Mona Lisa    |\n",
      "| ORGANIZATION | Louvre Museum  |\n",
      "|     GPE      |     Paris      |\n",
      "|     GPE      |     France     |\n",
      "|     GPE      |  Shakespeare   |\n",
      "| ORGANIZATION | United Nations |\n",
      "|     GPE      |    Leonardo    |\n",
      "|    PERSON    |     Vinci      |\n",
      "+--------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# Perform Named Entity Recognition for each sentence\n",
    "named_entities = []\n",
    "for sentence in sentences:\n",
    "    named_entities.extend(ner_with_nltk(sentence))\n",
    "\n",
    "# Initialize PrettyTable\n",
    "named_entity_table = PrettyTable([\"Entity\", \"Phrase Type\"])\n",
    "for entity, phrase in named_entities:\n",
    "    named_entity_table.add_row([entity, phrase])\n",
    "\n",
    "# Print the PrettyTable\n",
    "print(\"\\nNamed Entities:\")\n",
    "print(named_entity_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Curiosity Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:500;\">Q1. How does Named Entity Recognition (NER) contribute to information extraction in natural language processing?</p>\n",
    "Ans: Named Entity Recognition plays a crucial role in information extraction by identifying and classifying entities such as persons, organizations, locations, dates, and more in unstructured text. This helps in tasks like summarization, question answering, and knowledge graph construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:500;\">Q2. Challenges related to NER in real-life?</p>\n",
    "Ans: One challenge is ambiguity, where a word can have multiple possible entity types depending on context (e.g., \"Paris\" could refer to the city or a person's name). Another challenge is handling out-of-vocabulary entities or entities with diverse variations (e.g., person names with different spellings or titles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:500;\">Q3. How does chunking differ from Named Entity Recognition, and how are they related??</p>\n",
    "Ans: Chunking involves grouping adjacent words in a sentence into syntactic phrases, such as noun phrases (NP) or verb phrases (VP), without assigning specific semantic labels. Named Entity Recognition, on the other hand, specifically identifies and classifies named entities in text. While chunking focuses on syntactic structure, NER focuses on semantic meaning. However, NER can be considered a specialized form of chunking that targets named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:500;\">Q4. What are some potential applications of chunking and Named Entity Recognition?</p>\n",
    "Ans: Chunking can be applied in tasks such as information extraction, sentiment analysis, and machine translation by identifying and extracting meaningful phrases from text. Named Entity Recognition finds applications in information retrieval, document classification, and entity linking for organizing and retrieving information from large text corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:500;\">Q5. How do different approaches to Named Entity Recognition and chunking affect the accuracy and performance?</p>\n",
    "Ans: Various approaches, such as rule-based systems, statistical models, and deep learning methods, can be employed for Named Entity Recognition and chunking. Each approach has its strengths and weaknesses in terms of accuracy, scalability, and generalization to diverse text domains. Evaluating the performance of these approaches on benchmark datasets helps in understanding their effectiveness in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "In this experiment we learned about the concepts of chunking and Named Entity Recognition (NER) in natural language processing and their applications in information extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

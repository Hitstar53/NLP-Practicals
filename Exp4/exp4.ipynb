{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tf  {border-collapse:collapse;border-spacing:0;width:100%}\n",
    ".tf td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tf th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tf .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    ".col1 { width: 20%;}\n",
    ".col2 { width: 80%;}\n",
    "</style>\n",
    "<table class=\"tf\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Name</th>\n",
    "    <th class=\"tg-0pky col2\">Hatim Sawai</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky col1\">UID No.</td>\n",
    "    <td class=\"tg-0pky col2\">2021300108</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky col1\">Experiment No.</td>\n",
    "    <td class=\"tg-0pky col2\">4</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align:center;font-weight:500;\">Experiment 4</p>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;width:100%}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 10px;word-break:normal;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top;}\n",
    ".col1 { width: 20%;}\n",
    ".col2 { width: 80%;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Aim</th>\n",
    "    <th class=\"tg-0pky col2\">Classification using suitable classification model (NB).\n",
    "    </th>\n",
    "  </tr>\n",
    "</thead>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installation of NLTK and downloading the required corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
    "    text = text.replace(\"\\n\", \" \") # remove \\n\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {\n",
    "    0: \"Politics\",\n",
    "    1: \"Sport\",\n",
    "    2: \"Technology\",\n",
    "    3: \"Entertainment\",\n",
    "    4: \"Business\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_word_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.class_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for i in range(len(X)):\n",
    "            text = X.iloc[i]\n",
    "            label = y.iloc[i]\n",
    "            self.class_counts[label] += 1\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                self.class_word_counts[label][word] += 1\n",
    "                self.vocab.add(word)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        for i in range(len(X)):\n",
    "            text = X.iloc[i]\n",
    "            max_score = float(\"-inf\")\n",
    "            best_class = None\n",
    "            words = text.split()\n",
    "            class_probs = {}\n",
    "            for label in self.class_counts.keys():\n",
    "                score = np.log(\n",
    "                    self.class_counts[label] / sum(self.class_counts.values())\n",
    "                )\n",
    "                for word in words:\n",
    "                    count = self.class_word_counts[label][word] + 1\n",
    "                    total_count = len(self.vocab) + sum(\n",
    "                        self.class_word_counts[label].values()\n",
    "                    )\n",
    "                    score += np.log(count / total_count)\n",
    "                class_probs[label] = score\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    best_class = label\n",
    "            predictions.append(best_class)\n",
    "            probabilities.append(class_probs)\n",
    "        return predictions, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv(\"./Dataset/df_file.csv\")\n",
    "data[\"Text\"] = data[\"Text\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_data[\"Text\"], train_data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the classifier\n",
    "predictions, probabilities = classifier.predict(test_data[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier based on: Accuracy, Precision, Recall, F1 Score\n",
    "true_labels = test_data[\"label\"]\n",
    "accuracy = sum(predictions == true_labels) / len(true_labels)\n",
    "\n",
    "confusion_matrix = np.zeros((5, 5))\n",
    "for i in range(len(true_labels)):\n",
    "    confusion_matrix[true_labels.iloc[i], predictions[i]] += 1\n",
    "precision = np.zeros(5)\n",
    "recall = np.zeros(5)\n",
    "f1 = np.zeros(5)\n",
    "for i in range(5):\n",
    "    precision[i] = confusion_matrix[i, i] / sum(confusion_matrix[:, i])\n",
    "    recall[i] = confusion_matrix[i, i] / sum(confusion_matrix[i, :])\n",
    "    f1[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
    "    \n",
    "precision = np.mean(precision)\n",
    "recall = np.mean(recall)\n",
    "f1 = np.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    prediction, probability = classifier.predict(pd.Series([sentence]))\n",
    "    print(\"Probabilities:\")\n",
    "    for label, prob in probability[0].items():\n",
    "        print(f\"{label}: {np.exp(prob)}\")\n",
    "    print(\"Predicted Class:\", category_mapping[prediction[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9707865168539326\n",
      "Precision: 0.9693088273733436\n",
      "Recall: 0.9706189736139932\n",
      "F1 Score: 0.9698990525261447\n",
      "Confusion Matrix:\n",
      "[[89  0  1  0  2]\n",
      " [ 0 98  0  0  0]\n",
      " [ 1  0 74  2  0]\n",
      " [ 1  0  1 74  0]\n",
      " [ 2  0  2  1 97]]\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Predict a single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      "3: 1.1273390387023854e-26\n",
      "4: 2.2940067653534148e-24\n",
      "1: 1.099906355643834e-27\n",
      "0: 3.074579541547951e-27\n",
      "2: 1.7965457825652816e-25\n",
      "Predicted Class: Business\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Test prediction on a single input sentence\n",
    "input_sentence = \"The company has announced a new product launch.\"\n",
    "predict_single(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      "3: 4.53257744586028e-24\n",
      "4: 1.279231085662771e-22\n",
      "1: 1.444676989561495e-24\n",
      "0: 6.2252662487369875e-22\n",
      "2: 3.7607383704056025e-23\n",
      "Predicted Class: Politics\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"The government and the Army are in conflict.\"\n",
    "predict_single(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      "3: 6.109187318151378e-19\n",
      "4: 1.1082028232645113e-18\n",
      "1: 3.5351599308067194e-16\n",
      "0: 7.626843714808113e-19\n",
      "2: 5.960515007472776e-19\n",
      "Predicted Class: Sport\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"The football match was amazing.\"\n",
    "predict_single(input_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Curiosity Questions\n",
    "<p style=\"font-weight:500;\">Q1. What is the relation between accuracy and precision?</p>  \n",
    "Ans: Accuracy measures how close the result is to the actual value you were trying to achieve. Precision measures how close your results are to one another. While accuracy can be used in one instance, precision will be measured over time.\n",
    "\n",
    "**Mathematically:**\n",
    "<br>\n",
    "Accuracy is the ratio of correctly predicted observation to the total observations. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.  \n",
    "<br>\n",
    "Formulae as per confusion matrix:  \n",
    "Accuracy = (TP+TN)/(TP+FP+FN+TN)  \n",
    "Precision = TP/(TP+FP  \n",
    "Where,  \n",
    "TP = True Positive    \n",
    "TN = True Negative    \n",
    "FP = False Positive    \n",
    "FN = False Negative    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:500;\">Q2. Give example where precision is significant compared to accuracy?</p>\n",
    "Ans: In the case of spam detection, precision is more important than accuracy. If the precision is low, then the user will get a lot of spam messages in the inbox. But if the accuracy is low, then the user will get some spam messages in the inbox, but not as much as in the case of low precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:500;\">Q3. Give example where accuracy is significant compared to precision?</p>\n",
    "Ans: In the case of cancer detection, accuracy is more important than precision. If the accuracy is low, then the patient will not be diagnosed with cancer, but if the precision is low, then the patient will be diagnosed with cancer, but it may not be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "In this experiment we learned about the classification using Naive Bayes model. We also learned about the difference between accuracy and precision and their significance in different scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

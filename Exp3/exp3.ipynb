{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tf  {border-collapse:collapse;border-spacing:0;width:100%}\n",
    ".tf td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tf th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tf .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    ".col1 { width: 20%;}\n",
    ".col2 { width: 80%;}\n",
    "</style>\n",
    "<table class=\"tf\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Name</th>\n",
    "    <th class=\"tg-0pky col2\">Hatim Sawai</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky col1\">UID No.</td>\n",
    "    <td class=\"tg-0pky col2\">2021300108</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky col1\">Experiment No.</td>\n",
    "    <td class=\"tg-0pky col2\">3</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align:center;font-weight:500;\">Experiment 3</p>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;width:100%}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 10px;word-break:normal;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top;}\n",
    ".col1 { width: 20%;}\n",
    ".col2 { width: 80%;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Aim</th>\n",
    "    <th class=\"tg-0pky col2\">1. Calculate bigrams from a given corpus , display bigram probability table and calculate probability of a sentence.\n",
    "    <br>2. To apply add-one smoothing on sparse bigram table</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installation of NLTK and downloading the required corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatim\\AppData\\Local\\Temp\\ipykernel_1572\\1792132151.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "from prettytable import PrettyTable\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pattern.text.en import pluralize, conjugate, comparative\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hatim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hatim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hatim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing of the given corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = data.lower()\n",
    "    data = data.replace(\",\", \"\").replace(\".\", \" eos\").replace(\"!\", \" eos\").replace(\"?\", \" eos\").replace(\":\", \"\").replace(\";\", \"\").replace(\"'\", \"\").replace('\"', \"\")\n",
    "    data = data.replace(\"'s\", \"\")\n",
    "    data = \"eos \" + data\n",
    "    tokens = word_tokenize(data)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eos', 'you', 'book', 'a', 'flight', 'eos', 'i', 'read', 'a', 'book', 'eos', 'you', 'read', 'eos']\n"
     ]
    }
   ],
   "source": [
    "# Read the data from input.txt\n",
    "with open(\"input.txt\", \"r\") as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "# Preprocess the data\n",
    "tokens = preprocess(corpus)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Making Bigram Probability Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+------+-----+--------+------+------+\n",
      "|        | eos | you | book |  a  | flight |  i   | read |\n",
      "+--------+-----+-----+------+-----+--------+------+------+\n",
      "|  eos   | 0.0 | 0.5 | 0.0  | 0.0 |  0.0   | 0.25 | 0.0  |\n",
      "|  you   | 0.0 | 0.0 | 0.5  | 0.0 |  0.0   | 0.0  | 0.5  |\n",
      "|  book  | 0.5 | 0.0 | 0.0  | 0.5 |  0.0   | 0.0  | 0.0  |\n",
      "|   a    | 0.0 | 0.0 | 0.5  | 0.0 |  0.5   | 0.0  | 0.0  |\n",
      "| flight | 1.0 | 0.0 | 0.0  | 0.0 |  0.0   | 0.0  | 0.0  |\n",
      "|   i    | 0.0 | 0.0 | 0.0  | 0.0 |  0.0   | 0.0  | 1.0  |\n",
      "|  read  | 0.5 | 0.0 | 0.0  | 0.5 |  0.0   | 0.0  | 0.0  |\n",
      "+--------+-----+-----+------+-----+--------+------+------+\n"
     ]
    }
   ],
   "source": [
    "# create bigram table using pretty table\n",
    "unique = [\"\"] + tokens\n",
    "h = {}\n",
    "for i in unique:\n",
    "    h[i] = tokens.count(i)\n",
    "unique = pd.unique(unique)\n",
    "bigram_table = PrettyTable()\n",
    "bigram_table.field_names = [i for i in unique]\n",
    "unique = unique[1:]\n",
    "for i in unique:\n",
    "    row = [i]\n",
    "    for j in unique:\n",
    "        count = 0\n",
    "        for k in range(len(tokens) - 1):\n",
    "            if tokens[k] == i and tokens[k + 1] == j:\n",
    "                count += 1\n",
    "        p = count/h[i]\n",
    "        row.append(p)\n",
    "    bigram_table.add_row(row)\n",
    "\n",
    "print(bigram_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Making Add-One Smoothed Bigram Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+------+------+--------+------+------+\n",
      "|        | eos  | you  | book |  a   | flight |  i   | read |\n",
      "+--------+------+------+------+------+--------+------+------+\n",
      "|  eos   | 0.09 | 0.27 | 0.09 | 0.09 |  0.09  | 0.18 | 0.09 |\n",
      "|  you   | 0.11 | 0.11 | 0.22 | 0.11 |  0.11  | 0.11 | 0.22 |\n",
      "|  book  | 0.22 | 0.11 | 0.11 | 0.22 |  0.11  | 0.11 | 0.11 |\n",
      "|   a    | 0.11 | 0.11 | 0.22 | 0.11 |  0.22  | 0.11 | 0.11 |\n",
      "| flight | 0.25 | 0.12 | 0.12 | 0.12 |  0.12  | 0.12 | 0.12 |\n",
      "|   i    | 0.12 | 0.12 | 0.12 | 0.12 |  0.12  | 0.12 | 0.25 |\n",
      "|  read  | 0.22 | 0.11 | 0.11 | 0.22 |  0.11  | 0.11 | 0.11 |\n",
      "+--------+------+------+------+------+--------+------+------+\n"
     ]
    }
   ],
   "source": [
    "# create add one smoothing bigram table using pretty table\n",
    "unique = [\"\"] + tokens\n",
    "unique = pd.unique(unique)\n",
    "smooth_table = PrettyTable()\n",
    "smooth_table.field_names = [i for i in unique]\n",
    "unique = unique[1:]\n",
    "for i in unique:\n",
    "    row = [i]\n",
    "    for j in unique:\n",
    "        count = 0\n",
    "        for k in range(len(tokens) - 1):\n",
    "            if tokens[k] == i and tokens[k + 1] == j:\n",
    "                count += 1\n",
    "        p = (count+1)/(h[i]+len(unique))\n",
    "        p = round(p, 2)\n",
    "        row.append(p)\n",
    "    smooth_table.add_row(row)\n",
    "\n",
    "print(smooth_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculating Probability of a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = \"I book a flight.\"\n",
    "invalid = \"A flight book I.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eos', 'i', 'book', 'a', 'flight', 'eos']\n",
      "['eos', 'a', 'flight', 'book', 'i', 'eos']\n"
     ]
    }
   ],
   "source": [
    "tokensa = preprocess(valid)\n",
    "tokensb = preprocess(invalid)\n",
    "print(tokensa)\n",
    "print(tokensb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilty of sentence A: 0.00026136\n",
      "Probabilty of sentence B: 3.13632e-05\n"
     ]
    }
   ],
   "source": [
    "proba, probb = 1, 1\n",
    "for i in range(len(tokensa) - 1):\n",
    "    proba *= smooth_table.rows[smooth_table.field_names.index(tokensa[i])-1][smooth_table.field_names.index(tokensa[i+1])]\n",
    "\n",
    "for i in range(len(tokensb) - 1):\n",
    "    probb *= smooth_table.rows[smooth_table.field_names.index(tokensb[i])-1][smooth_table.field_names.index(tokensb[i+1])]\n",
    "\n",
    "print(\"Probabilty of sentence A:\", proba)\n",
    "print(\"Probabilty of sentence B:\", probb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence A is more probable\n"
     ]
    }
   ],
   "source": [
    "if proba > probb:\n",
    "    print(\"Sentence A is more probable\")\n",
    "else:\n",
    "    print(\"Sentence B is more probable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Curiosity Questions\n",
    "<p style=\"font-weight:500;\">Q1. Some Terminologies</p>  \n",
    " \n",
    "1. **Corpus**: A corpus is a collection of written texts and is used for language research and development.  \n",
    "2. **Bigram**: A bigram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2.  \n",
    "3. **Add-one Smoothing**: Add-one smoothing is a simple method to smooth zero probabilities in a probability distribution. It is also known as Laplace smoothing.  \n",
    "4. **Sparse Table**: A sparse table is a table that has a large number of cells with zero values. It is a table in which most of the entries are zero.  \n",
    "5. **Word Form:** the inflected form as it actually appears in the corpus.  \n",
    "6. **Lemma:** an abstract form, shared by word forms having the same stem, part of speech, and word sense â€“ stands for the class of words with stem.  \n",
    "7. **Types:** number of distinct words in a corpus (vocabulary size).  \n",
    "8. **Tokens:** total number of words in a corpus.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:500;\">Q2. What is Bi-gram Model? How to calculate the probability of a sentence?</p>\n",
    "Ans: A bigram model is a type of n-gram language model where \"n\" equals 2. In simpler terms, it predicts the next word in a sentence based solely on the word that came before it. This assumption, termed the Markov assumption, states that the probability of a word depends only on the previous word, ignoring all preceding context. While simplistic, bigram models hold historical significance as the foundation for more complex language models.  \n",
    "\n",
    "**Calculating Sentence Probability:**  \n",
    "Here's how to calculate the probability of a sentence in a bigram model:  \n",
    "1. Tokenize the sentence: Divide the sentence into individual words (tokens).\n",
    "2. Estimate bigram probabilities: Calculate the probability of each word following the previous word in the training corpus. This uses the formula: **P(w_i | w_(i-1)) = Count(w_(i-1), w_i) / Count(w_(i-1))**    \n",
    "\n",
    "where:  \n",
    "P(w_i | w_(i-1)) is the probability of word w_i given the previous word w_(i-1).  \n",
    "Count(w_(i-1), w_i) is the number of times the bigram (w_(i-1), w_i) appears in the training corpus.  \n",
    "Count(w_(i-1)) is the number of times the word w_(i-1) appears in the training corpus.  \n",
    "Multiply probabilities: Multiply the individual bigram probabilities for each word pair in the sentence.  \n",
    "\n",
    "3. Smoothing: Since most word pairs might not be present in the corpus, apply smoothing techniques like Laplace smoothing or Witten-Bell discounting to adjust probabilities and avoid zero probabilities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "In this experiment we learned about bigrams and add-one smoothing. We calculated bigrams from a given corpus and displayed the bigram probability table. We also calculated the probability of a sentence using bigram probabilities. We also applied add-one smoothing on the sparse bigram table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
